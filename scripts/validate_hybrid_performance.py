#!/usr/bin/env python3
"""
ValidaÃ§Ã£o de Performance do Sistema HÃ­brido
Testa se a migraÃ§Ã£o para permissÃµes granulares nÃ£o impacta performance
"""

import asyncio
import logging
import statistics
import sys
import time
from typing import Any, Dict, List
from unittest.mock import AsyncMock, patch

# Adicionar o caminho da aplicaÃ§Ã£o
sys.path.append("/home/juliano/Projetos/pro_team_care_16")

from app.domain.entities.user import User
from app.infrastructure.cache.permission_cache import PermissionCache
from app.presentation.decorators.hybrid_permissions import check_user_permission

# Setup logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class PerformanceValidator:
    """Validador de performance do sistema hÃ­brido"""

    def __init__(self):
        self.cache = PermissionCache()
        self.results = {}

    async def setup(self):
        """Setup inicial"""
        logger.info("ðŸš€ Iniciando validaÃ§Ã£o de performance...")
        # Inicializar cache sem Redis (teste local)
        self.cache.redis = None

    def create_test_user(self, user_id: int, level: int) -> User:
        """Criar usuÃ¡rio de teste"""
        return User(
            id=user_id,
            email_address=f"test{user_id}@example.com",
            user_name=f"testuser{user_id}",
            level=level,
            company_id=1,
            establishment_id=1,
            is_system_admin=False,
        )

    async def benchmark_permission_check(
        self, iterations: int = 1000
    ) -> Dict[str, float]:
        """Benchmark de verificaÃ§Ã£o de permissÃµes"""
        logger.info(
            f"ðŸ“Š Executando benchmark de permissÃµes ({iterations} iteraÃ§Ãµes)..."
        )

        user = self.create_test_user(1, 60)
        permission = "users.view"
        context_type = "establishment"

        # Mock do cache para retornar permissÃµes
        mock_permissions = ["users.view", "users.list", "companies.view"]

        with patch.object(
            self.cache, "get_user_permissions", return_value=mock_permissions
        ):
            # Benchmark: verificaÃ§Ã£o via permissÃµes granulares
            start_time = time.perf_counter()
            for _ in range(iterations):
                await self.cache.has_permission(user.id, permission, context_type)
            granular_time = time.perf_counter() - start_time

        # Benchmark: verificaÃ§Ã£o via nÃ­veis (simulaÃ§Ã£o)
        start_time = time.perf_counter()
        for _ in range(iterations):
            # Simula verificaÃ§Ã£o simples de nÃ­vel
            result = user.level >= 40  # Equivalente ao nÃ­vel mÃ­nimo
        level_time = time.perf_counter() - start_time

        logger.info(
            f"  âš¡ Granular: {granular_time:.4f}s ({granular_time/iterations*1000:.2f}ms/op)"
        )
        logger.info(
            f"  âš¡ NÃ­vel:    {level_time:.4f}s ({level_time/iterations*1000:.2f}ms/op)"
        )
        logger.info(f"  ðŸ“ˆ Overhead: {((granular_time/level_time - 1) * 100):.1f}%")

        return {
            "granular_time": granular_time,
            "level_time": level_time,
            "overhead_percent": ((granular_time / level_time - 1) * 100),
            "iterations": iterations,
        }

    async def benchmark_cache_performance(
        self, iterations: int = 500
    ) -> Dict[str, float]:
        """Benchmark do cache de permissÃµes"""
        logger.info(f"ðŸ’¾ Testando performance do cache ({iterations} iteraÃ§Ãµes)...")

        user = self.create_test_user(2, 70)
        permission = "companies.edit"
        context_type = "company"

        # Mock do banco de dados (simulando busca lenta)
        mock_permissions = ["companies.edit", "companies.view", "establishments.edit"]

        with patch.object(
            self.cache, "_fetch_permissions_from_db", return_value=mock_permissions
        ) as mock_db:
            # Primeira chamada (cache miss)
            start_time = time.perf_counter()
            await self.cache.get_user_permissions(user.id, context_type)
            first_call_time = time.perf_counter() - start_time

            # Chamadas subsequentes (cache hit simulado)
            times = []
            for _ in range(iterations):
                start_time = time.perf_counter()
                await self.cache.get_user_permissions(user.id, context_type)
                times.append(time.perf_counter() - start_time)

            avg_cached_time = statistics.mean(times)
            speedup = first_call_time / avg_cached_time

            logger.info(f"  ðŸ” Primeira busca (DB):  {first_call_time*1000:.2f}ms")
            logger.info(f"  âš¡ Cache hit mÃ©dio:      {avg_cached_time*1000:.4f}ms")
            logger.info(f"  ðŸš€ Speedup:              {speedup:.1f}x")

            # Mock deve ter sido chamado apenas uma vez
            assert (
                mock_db.call_count == 1
            ), f"DB deveria ser chamado 1x, foi {mock_db.call_count}x"

        return {
            "first_call_ms": first_call_time * 1000,
            "cached_call_ms": avg_cached_time * 1000,
            "speedup": speedup,
            "iterations": iterations,
        }

    async def benchmark_hybrid_decorator(
        self, iterations: int = 1000
    ) -> Dict[str, float]:
        """Benchmark do decorator hÃ­brido"""
        logger.info(
            f"ðŸŽ­ Testando performance do decorator hÃ­brido ({iterations} iteraÃ§Ãµes)..."
        )

        user = self.create_test_user(3, 80)
        permission = "roles.create"
        min_level = 70
        context_type = "establishment"

        # Mock para retornar que usuÃ¡rio TEM a permissÃ£o (caso otimista)
        with patch.object(self.cache, "has_permission", return_value=True):
            start_time = time.perf_counter()
            for _ in range(iterations):
                # Simular a lÃ³gica do decorator
                has_perm = await self.cache.has_permission(
                    user.id, permission, context_type
                )
                if not has_perm:
                    # Fallback para nÃ­vel
                    has_perm = user.level >= min_level
            permission_path_time = time.perf_counter() - start_time

        # Mock para retornar que usuÃ¡rio NÃƒO tem a permissÃ£o (fallback para nÃ­vel)
        with patch.object(self.cache, "has_permission", return_value=False):
            start_time = time.perf_counter()
            for _ in range(iterations):
                # Simular a lÃ³gica do decorator
                has_perm = await self.cache.has_permission(
                    user.id, permission, context_type
                )
                if not has_perm:
                    # Fallback para nÃ­vel
                    has_perm = user.level >= min_level
            fallback_path_time = time.perf_counter() - start_time

        logger.info(
            f"  âœ… Caminho permissÃ£o:    {permission_path_time*1000/iterations:.3f}ms/op"
        )
        logger.info(
            f"  ðŸ”„ Caminho fallback:     {fallback_path_time*1000/iterations:.3f}ms/op"
        )

        return {
            "permission_path_ms": permission_path_time * 1000 / iterations,
            "fallback_path_ms": fallback_path_time * 1000 / iterations,
            "iterations": iterations,
        }

    async def test_memory_usage(self) -> Dict[str, Any]:
        """Teste de uso de memÃ³ria"""
        logger.info("ðŸ§  Testando uso de memÃ³ria...")

        import os

        import psutil

        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        # Simular cache com muitos usuÃ¡rios
        users_count = 100
        permissions_per_user = 20

        mock_permissions = [f"permission_{i}" for i in range(permissions_per_user)]

        with patch.object(
            self.cache, "_fetch_permissions_from_db", return_value=mock_permissions
        ):
            # PrÃ©-carregar cache para mÃºltiplos usuÃ¡rios
            for user_id in range(1, users_count + 1):
                await self.cache.get_user_permissions(user_id, "establishment")

        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory

        logger.info(f"  ðŸ“Š MemÃ³ria inicial:      {initial_memory:.1f} MB")
        logger.info(f"  ðŸ“Š MemÃ³ria final:        {final_memory:.1f} MB")
        logger.info(f"  ðŸ“ˆ Aumento:              {memory_increase:.1f} MB")
        logger.info(f"  ðŸ‘¥ UsuÃ¡rios simulados:   {users_count}")
        logger.info(f"  ðŸ’¾ MB por usuÃ¡rio:       {memory_increase/users_count:.3f} MB")

        return {
            "initial_memory_mb": initial_memory,
            "final_memory_mb": final_memory,
            "memory_increase_mb": memory_increase,
            "users_count": users_count,
            "mb_per_user": memory_increase / users_count,
        }

    async def validate_backward_compatibility(self) -> Dict[str, bool]:
        """Validar compatibilidade com sistema legado"""
        logger.info("ðŸ”„ Validando compatibilidade com sistema legado...")

        results = {}

        # Teste 1: UsuÃ¡rio com nÃ­vel alto deve conseguir acessar (mesmo sem permissÃ£o granular)
        high_level_user = self.create_test_user(4, 90)
        with patch.object(self.cache, "has_permission", return_value=False):
            # Simular verificaÃ§Ã£o hÃ­brida
            has_access = await self.cache.has_permission(
                high_level_user.id, "system.admin", "system"
            )
            if not has_access:
                has_access = high_level_user.level >= 90  # Fallback

        results["high_level_fallback"] = has_access
        logger.info(
            f"  âœ… UsuÃ¡rio nÃ­vel alto (fallback): {'âœ…' if has_access else 'âŒ'}"
        )

        # Teste 2: UsuÃ¡rio com permissÃ£o granular deve conseguir acessar (mesmo com nÃ­vel baixo)
        low_level_user = self.create_test_user(5, 30)
        with patch.object(self.cache, "has_permission", return_value=True):
            has_access = await self.cache.has_permission(
                low_level_user.id, "users.view", "establishment"
            )

        results["granular_permission_works"] = has_access
        logger.info(
            f"  âœ… UsuÃ¡rio com permissÃ£o granular: {'âœ…' if has_access else 'âŒ'}"
        )

        # Teste 3: Sistema deve negar acesso quando nem nÃ­vel nem permissÃ£o sÃ£o suficientes
        low_user = self.create_test_user(6, 20)
        with patch.object(self.cache, "has_permission", return_value=False):
            has_access = await self.cache.has_permission(
                low_user.id, "system.admin", "system"
            )
            if not has_access:
                has_access = low_user.level >= 90  # Fallback

        results["proper_denial"] = not has_access  # Deve ser False (acesso negado)
        logger.info(
            f"  âœ… NegaÃ§Ã£o correta de acesso: {'âœ…' if not has_access else 'âŒ'}"
        )

        return results

    async def generate_performance_report(self) -> Dict[str, Any]:
        """Gerar relatÃ³rio completo de performance"""
        logger.info("ðŸ“‹ Gerando relatÃ³rio de performance...")

        report = {}

        # Executar todos os benchmarks
        report["permission_benchmark"] = await self.benchmark_permission_check()
        report["cache_performance"] = await self.benchmark_cache_performance()
        report["hybrid_decorator"] = await self.benchmark_hybrid_decorator()
        report["memory_usage"] = await self.test_memory_usage()
        report["compatibility"] = await self.validate_backward_compatibility()

        # AnÃ¡lise geral
        permission_overhead = report["permission_benchmark"]["overhead_percent"]
        cache_speedup = report["cache_performance"]["speedup"]
        memory_per_user = report["memory_usage"]["mb_per_user"]

        report["analysis"] = {
            "performance_acceptable": permission_overhead < 50,  # Overhead < 50%
            "cache_effective": cache_speedup > 10,  # Cache speedup > 10x
            "memory_efficient": memory_per_user < 1.0,  # < 1MB por usuÃ¡rio
            "compatibility_ok": all(report["compatibility"].values()),
        }

        overall_ok = all(report["analysis"].values())
        report["overall_status"] = "âœ… APROVADO" if overall_ok else "âŒ REQUER ATENÃ‡ÃƒO"

        return report


async def main():
    """FunÃ§Ã£o principal de validaÃ§Ã£o"""
    logger.info("=" * 60)
    logger.info("ðŸš€ VALIDAÃ‡ÃƒO DE PERFORMANCE - SISTEMA HÃBRIDO")
    logger.info("=" * 60)

    validator = PerformanceValidator()
    await validator.setup()

    try:
        # Gerar relatÃ³rio completo
        report = await validator.generate_performance_report()

        # Exibir resultados
        logger.info("\n" + "=" * 60)
        logger.info("ðŸ“Š RELATÃ“RIO DE PERFORMANCE")
        logger.info("=" * 60)

        # Performance de permissÃµes
        perm_bench = report["permission_benchmark"]
        logger.info(f"ðŸ” VerificaÃ§Ã£o de PermissÃµes:")
        logger.info(f"   Overhead vs nÃ­veis: {perm_bench['overhead_percent']:.1f}%")

        # Cache performance
        cache_perf = report["cache_performance"]
        logger.info(f"ðŸ’¾ Performance do Cache:")
        logger.info(f"   Speedup: {cache_perf['speedup']:.1f}x")
        logger.info(f"   Cache hit: {cache_perf['cached_call_ms']:.3f}ms")

        # MemÃ³ria
        memory = report["memory_usage"]
        logger.info(f"ðŸ§  Uso de MemÃ³ria:")
        logger.info(f"   Por usuÃ¡rio: {memory['mb_per_user']:.3f} MB")

        # Compatibilidade
        compat = report["compatibility"]
        logger.info(f"ðŸ”„ Compatibilidade:")
        logger.info(f"   Todos os testes: {'âœ…' if all(compat.values()) else 'âŒ'}")

        # Status geral
        analysis = report["analysis"]
        logger.info(f"\nðŸŽ¯ ANÃLISE GERAL:")
        logger.info(
            f"   Performance: {'âœ…' if analysis['performance_acceptable'] else 'âŒ'}"
        )
        logger.info(f"   Cache:       {'âœ…' if analysis['cache_effective'] else 'âŒ'}")
        logger.info(f"   MemÃ³ria:     {'âœ…' if analysis['memory_efficient'] else 'âŒ'}")
        logger.info(f"   Compatib.:   {'âœ…' if analysis['compatibility_ok'] else 'âŒ'}")

        logger.info(f"\nðŸ† STATUS FINAL: {report['overall_status']}")

        if report["overall_status"] == "âœ… APROVADO":
            logger.info("\nâœ… Sistema hÃ­brido validado com sucesso!")
            logger.info("   âœ… Performance dentro dos limites aceitÃ¡veis")
            logger.info("   âœ… Cache funcionando eficientemente")
            logger.info("   âœ… Uso de memÃ³ria otimizado")
            logger.info("   âœ… Compatibilidade mantida")
            logger.info("\nðŸš€ PRONTO PARA PRODUÃ‡ÃƒO!")
            return 0
        else:
            logger.error("\nâŒ Sistema requer otimizaÃ§Ãµes antes da produÃ§Ã£o")
            return 1

    except Exception as e:
        logger.error(f"ðŸ’¥ Erro na validaÃ§Ã£o: {e}")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
